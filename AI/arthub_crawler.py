#arthub_crawler.py
import requests
from bs4 import BeautifulSoup
import time

def get_arthub_detail(link):
    """
    ìƒì„¸ í˜ì´ì§€ URLë¡œ ì ‘ì†í•´ì„œ ê°€ê²©ê³¼ ë³¸ë¬¸ ë‚´ìš©ì„ ê¸ì–´ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        res = requests.get(link)
        res.encoding = 'euc-kr'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ì•„íŠ¸í—ˆë¸Œ ìƒì„¸ í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ (ê°œë°œì ë„êµ¬ F12 ê¸°ë°˜)
        # ë³¸ë¬¸ ë‚´ìš©ì€ ë³´í†µ 'board_view_contents' í´ë˜ìŠ¤ ì•ˆì— ìˆìŒ
        content_div = soup.select_one(".board_view_contents")
        
        if content_div:
            # HTML íƒœê·¸ ë‹¤ ë–¼ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
            full_text = content_div.get_text(separator=" ", strip=True)
            # ë„ˆë¬´ ê¸°ë‹ˆê¹Œ ì•ë¶€ë¶„ 300ìë§Œ (AI ìš”ì•½ìš©)
            return full_text[:500] 
        else:
            return "ìƒì„¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception:
        return "í¬ë¡¤ë§ ì‹¤íŒ¨"

def crawl_arthub():
    url = "https://www.arthub.co.kr/sub01/board03_list.htm"
    params = {'k_area': 'A'} 
    response = requests.get(url, params=params)
    response.encoding = 'euc-kr'
    soup = BeautifulSoup(response.text, 'html.parser')
    
    items = soup.select(".list_type_01") 
    results = []

    print(f"ğŸ” ì•„íŠ¸í—ˆë¸Œ í¬ë¡¤ë§ ì‹œì‘ ({len(items)}ê°œ)...")

    for item in items:
        try:
            title_tag = item.select_one(".subject a")
            title = title_tag.text.strip()
            link = "https://www.arthub.co.kr/sub01/" + title_tag['href']
            
            # [í•µì‹¬] ìƒì„¸ í˜ì´ì§€ë¡œ ë“¤ì–´ê°€ì„œ ë‚´ìš© ê¸ì–´ì˜¤ê¸°
            print(f"  > ìƒì„¸ ê¸ëŠ” ì¤‘: {title}...")
            detail_desc = get_arthub_detail(link)
            
            # ê°€ê²© ì •ë³´ëŠ” ì•„íŠ¸í—ˆë¸Œ ëª©ë¡ì—” ì˜ ì—†ê³  ìƒì„¸ ë³¸ë¬¸ì— í¬í•¨ëœ ê²½ìš°ê°€ ë§ìŒ.
            # ì •í˜•í™”í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ 'ë¬´ë£Œ/ìœ ë£Œ'ëŠ” ë³¸ë¬¸ ë¶„ì„ì´ë‚˜ ë³„ë„ ë¡œì§ í•„ìš”.
            # ì¼ë‹¨ì€ 'ë³„ë„ ë¬¸ì˜'ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ë³¸ë¬¸ì—ì„œ 'ì›'ì„ ì°¾ëŠ” ë¡œì§ ì¶”ê°€ ê°€ëŠ¥.
            
            data = {
                "title": title,
                "place": item.select_one(".date").text.split('|')[-1].strip(),
                "period": item.select_one(".date").text.split('|')[0].strip(),
                "image": item.select_one(".img_box img")['src'],
                "price": "ìƒì„¸í˜ì´ì§€ ì°¸ì¡°", # ì•„íŠ¸í—ˆë¸ŒëŠ” ê°€ê²© ì¹¸ì´ ë”°ë¡œ ì—†ìŒ
                "desc": detail_desc,        # ë³¸ë¬¸ ë‚´ìš© (AI ì¶”ì²œì— ì‚¬ìš©)
                "link": link
            }
            results.append(data)
            time.sleep(0.5) # ì°¨ë‹¨ ë°©ì§€ìš© ë”œë ˆì´
            
        except Exception as e:
            continue
            
    return results